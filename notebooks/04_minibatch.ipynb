{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch import tensor,nn\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "path_data = Path('../data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, features = x_train.shape\n",
    "predictionCategories = y_train.max()+1\n",
    "nh = 50\n",
    "\n",
    "lr = 0.2\n",
    "epochs = 2\n",
    "bs = 50\n",
    "predictionCategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss \n",
    "\n",
    "In the case of one hot encoded answer vectors, cross entropy loss is really just `-log(pi)`  where pi is the prediction value of the target.  A softmax must be completed before the loss calculation step.\n",
    "\n",
    "All this is contained in `F.cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An example of how nn.Module registers added children as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My Numbers: , 1, 2, 3'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [1, 2, 3]\n",
    "reduce(lambda x,y: f'{x}, {y}', my_list, 'My Numbers: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule:\n",
    "    def __init__(self):\n",
    "        self._modules = {}\n",
    "    \n",
    "    def __setattr__(self, k, v):\n",
    "        if not k.startswith('_'): self._modules[k] = v\n",
    "        super().__setattr__(k, v)\n",
    "    \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values(): yield from l.parameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return reduce(lambda val, layer: layer(val), self.layers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'layer1': Linear(in_features=1, out_features=2, bias=True)},\n",
       " [Parameter containing:\n",
       "  tensor([[ 0.52],\n",
       "          [-0.44]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.19,  0.47], requires_grad=True)])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModule = MyModule()\n",
    "myModule.layer1 = nn.Linear(1, 2)\n",
    "myModule, list(myModule.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All of the above code is contained in nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(): \n",
    "    def __init__(self, params, lr=0.5): \n",
    "        self.params, self.lr = list(params), lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * self.lr\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(features, nh), nn.ReLU(), nn.Linear(nh, predictionCategories))\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16, 0.94\n",
      "0.15, 0.92\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This optimizer is already implemented by optim.SGD which also includes additional features like momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(features, nh), nn.ReLU(), nn.Linear(nh, predictionCategories))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterTest():\n",
    "    def __init__(self): self.vals = [1, 2, 3, 4]\n",
    "    def __iter__(self): \n",
    "        for v in self.vals: yield f'value: {v}'\n",
    "        \n",
    "# class IterTest2():\n",
    "#     def __init__(self): self.iterTest, self.letters = IterTest(), ['a', 'b', 'c']\n",
    "#     def __iter__(self): \n",
    "#        yield from ( for b in self.iterTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 1\n",
      "value: 2\n",
      "['value: 3', 'value: 4']\n"
     ]
    }
   ],
   "source": [
    "testIter = iter(IterTest())\n",
    "print(next(testIter))\n",
    "print(next(testIter))\n",
    "print(list(testIter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      " \u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstore_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Store params named in comma-separated `names` from calling context into attrs in `self`\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/fastai/lib/python3.11/site-packages/fastcore/basics.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "? fc.store_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m  \u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_sz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Return batches from iterator `it` of size `chunk_sz` (or return `n_chunks` total)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/fastai/lib/python3.11/site-packages/fastcore/basics.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "? fc.chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m  \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Shuffle list x in place, and return None.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/fastai/lib/python3.11/random.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "? random.shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "range(stop) -> range object\n",
      "range(start, stop[, step]) -> range object\n",
      "\n",
      "Return an object that produces a sequence of integers from start (inclusive)\n",
      "to stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\n",
      "start defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\n",
      "These are exactly the valid indices for a list of 4 elements.\n",
      "When step is given, it specifies the increment (or decrement).\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "? range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.07,\n",
       "         0.07, 0.07, 0.49, 0.53, 0.68, 0.10, 0.65, 1.00, 0.96, 0.50, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.12, 0.14, 0.37, 0.60, 0.66, 0.99, 0.99, 0.99, 0.99, 0.99, 0.88, 0.67, 0.99, 0.95, 0.76, 0.25, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.19, 0.93, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.36, 0.32, 0.32, 0.22, 0.15, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.86, 0.99, 0.99, 0.99, 0.99, 0.99, 0.77, 0.71, 0.96, 0.94,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.31, 0.61, 0.42, 0.99,\n",
       "         0.99, 0.80, 0.04, 0.00, 0.17, 0.60, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.05, 0.00, 0.60, 0.99, 0.35, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.54, 0.99, 0.74, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.74, 0.99, 0.27, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.14, 0.94, 0.88, 0.62, 0.42, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.32, 0.94, 0.99, 0.99, 0.46, 0.10, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.73, 0.99, 0.99, 0.59, 0.11,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.06, 0.36, 0.98, 0.99, 0.73, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.97, 0.99, 0.97, 0.25, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.51, 0.71, 0.99, 0.99, 0.81, 0.01, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15, 0.58, 0.89, 0.99, 0.99, 0.99,\n",
       "         0.98, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.09, 0.45,\n",
       "         0.86, 0.99, 0.99, 0.99, 0.99, 0.79, 0.30, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.09, 0.26, 0.83, 0.99, 0.99, 0.99, 0.99, 0.77, 0.32, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.67, 0.86, 0.99, 0.99, 0.99, 0.99, 0.76, 0.31, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.21, 0.67, 0.88, 0.99, 0.99, 0.99, 0.99, 0.95, 0.52, 0.04, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.53, 0.99, 0.99, 0.99, 0.83, 0.53,\n",
       "         0.52, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, shuffle=False): self.n, self.shuffle = len(ds), shuffle\n",
    "    def __iter__(self):\n",
    "        res = list(range(self.n))\n",
    "        if(self.shuffle): random.shuffle(res)\n",
    "        return iter(res)\n",
    "\n",
    "class BatchSampler():\n",
    "    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n",
    "    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)\n",
    "\n",
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, batches, collate_fn=collate):fc.store_attr()\n",
    "    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badCollate(b): return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(Sampler(train_ds, shuffle=True), bs)\n",
    "valid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)\n",
    "train_dl = DataLoader(train_ds, train_samp)\n",
    "valid_dl = DataLoader(valid_ds, valid_samp)\n",
    "bad_train_dl = DataLoader(train_ds, train_samp, collate_fn=badCollate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "zip(*iterables, strict=False) --> Yield tuples until an input is exhausted.\n",
      "\n",
      "   >>> list(zip('abcdefg', range(3), range(4)))\n",
      "   [('a', 0, 0), ('b', 1, 1), ('c', 2, 2)]\n",
      "\n",
      "The zip object yields n-length tuples, where n is the number of iterables\n",
      "passed as positional arguments to zip().  The i-th element in every tuple\n",
      "comes from the i-th iterable argument to zip().  This continues until the\n",
      "shortest argument is exhausted.\n",
      "\n",
      "If strict is true and one of the arguments is exhausted before the others,\n",
      "raise a ValueError.\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "? zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "1 2 3\n",
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "print(a)\n",
    "print(*a)\n",
    "print(a[0], a[1], a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6, 7, 3, 4, 2, 3, 5, 2, 4, 1, 6, 2, 0, 0, 4, 6, 0, 3, 5, 5, 7, 6, 3, 7, 4, 2, 6, 5, 4, 1, 6, 5, 8, 5, 8, 0, 7, 4, 3, 6, 5, 5, 6,\n",
       "        7, 8, 8, 5, 9, 9])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = zip(*next(iter(bad_train_dl)))\n",
    "torch.stack(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 3, 7, 4, 2, 2, 3, 7, 0, 9, 3, 1, 3, 1, 9, 7, 6, 9, 8, 2, 4, 7, 5, 5, 1, 8, 1, 6, 4, 9, 0, 6, 1, 4, 6, 8, 1, 0, 9, 2, 1, 5, 5, 7,\n",
       "         2, 0, 4, 1, 2, 2]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[next(iter(train_samp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 3, 7, 4, 9, 8, 6, 2, 1, 8, 4, 2, 4, 4, 2, 4, 0, 2, 0, 5, 6, 4, 2, 2, 1, 9, 9, 0, 8, 3, 6, 9, 7, 7, 6, 3, 0, 8, 7, 1, 4, 6, 5, 5,\n",
       "         3, 2, 8, 7, 5, 5]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate(train_ds[i] for i in next(iter(train_samp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc,count = 0.,0.,0\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred,yb).item()*n\n",
    "                tot_acc  += accuracy (pred,yb).item()*n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(RandomSampler(train_ds), bs, drop_last=False)\n",
    "valid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds)\n",
    "valid_dl = DataLoader(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, shuffle=False, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.19284493958577514 0.946200003027916\n",
      "1 0.15838701024651528 0.9539000064134597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.15838701024651528, 0.9539000064134597)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
