{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch import tensor,nn\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "path_data = Path('../data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, features = x_train.shape\n",
    "predictionCategories = y_train.max()+1\n",
    "nh = 50\n",
    "\n",
    "lr = 0.2\n",
    "epochs = 5\n",
    "bs = 50\n",
    "predictionCategories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss \n",
    "\n",
    "In the case of one hot encoded answer vectors, cross entropy loss is really just `-log(pi)`  where pi is the prediction value of the target.  A softmax must be completed before the loss calculation step.\n",
    "\n",
    "All this is contained in `F.cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def report(loss, preds, yb): print(f'{loss:.2f}, {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An example of how nn.Module registers added children as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My Numbers: , 1, 2, 3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [1, 2, 3]\n",
    "reduce(lambda x,y: f'{x}, {y}', my_list, \"My Numbers: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule:\n",
    "    def __init__(self):\n",
    "        self._modules = {}\n",
    "    \n",
    "    def __setattr__(self, k, v):\n",
    "        if not k.startswith(\"_\"): self._modules[k] = v\n",
    "        super().__setattr__(k, v)\n",
    "    \n",
    "    def __repr__(self): return f'{self._modules}'\n",
    "    \n",
    "    def parameters(self):\n",
    "        for l in self._modules.values(): yield from l.parameters()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return reduce(lambda val, layer: layer(val), self.layers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'layer1': Linear(in_features=1, out_features=2, bias=True)},\n",
       " [Parameter containing:\n",
       "  tensor([[ 0.52],\n",
       "          [-0.44]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.19,  0.47], requires_grad=True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModule = MyModule()\n",
    "myModule.layer1 = nn.Linear(1, 2)\n",
    "myModule, list(myModule.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All of the above code is contained in nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(): \n",
    "    def __init__(self, params, lr=0.5): \n",
    "        self.params, self.lr = list(params), lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * self.lr\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(features, nh), nn.ReLU(), nn.Linear(nh, predictionCategories))\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16, 0.94\n",
      "0.15, 0.92\n",
      "0.14, 0.92\n",
      "0.13, 0.92\n",
      "0.11, 0.96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This optimizer is already implemented by optim.SGD which also includes additional features like momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(features, nh), nn.ReLU(), nn.Linear(nh, predictionCategories))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterTest():\n",
    "    def __init__(self): self.vals = [1, 2, 3, 4]\n",
    "    def __iter__(self): \n",
    "        for v in self.vals: yield f'value: {v}'\n",
    "        \n",
    "# class IterTest2():\n",
    "#     def __init__(self): self.iterTest, self.letters = IterTest(), ['a', 'b', 'c']\n",
    "#     def __iter__(self): \n",
    "#        yield from ( for b in self.iterTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 1\n",
      "value: 2\n",
      "['value: 3', 'value: 4']\n"
     ]
    }
   ],
   "source": [
    "testIter = iter(IterTest())\n",
    "print(next(testIter))\n",
    "print(next(testIter))\n",
    "print(list(testIter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      " \u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstore_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Store params named in comma-separated `names` from calling context into attrs in `self`\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/fastai/lib/python3.11/site-packages/fastcore/basics.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "? fc.store_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m  \u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_sz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Return batches from iterator `it` of size `chunk_sz` (or return `n_chunks` total)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/fastai/lib/python3.11/site-packages/fastcore/basics.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "? fc.chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m  \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Shuffle list x in place, and return None.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/fastai/lib/python3.11/random.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "? random.shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "range(stop) -> range object\n",
      "range(start, stop[, step]) -> range object\n",
      "\n",
      "Return an object that produces a sequence of integers from start (inclusive)\n",
      "to stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\n",
      "start defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\n",
      "These are exactly the valid indices for a list of 4 elements.\n",
      "When step is given, it specifies the increment (or decrement).\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "? range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.07,\n",
       "         0.07, 0.07, 0.49, 0.53, 0.68, 0.10, 0.65, 1.00, 0.96, 0.50, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.12, 0.14, 0.37, 0.60, 0.66, 0.99, 0.99, 0.99, 0.99, 0.99, 0.88, 0.67, 0.99, 0.95, 0.76, 0.25, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.19, 0.93, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.98, 0.36, 0.32, 0.32, 0.22, 0.15, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.86, 0.99, 0.99, 0.99, 0.99, 0.99, 0.77, 0.71, 0.96, 0.94,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.31, 0.61, 0.42, 0.99,\n",
       "         0.99, 0.80, 0.04, 0.00, 0.17, 0.60, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.05, 0.00, 0.60, 0.99, 0.35, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.54, 0.99, 0.74, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.74, 0.99, 0.27, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.14, 0.94, 0.88, 0.62, 0.42, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.32, 0.94, 0.99, 0.99, 0.46, 0.10, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.73, 0.99, 0.99, 0.59, 0.11,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.06, 0.36, 0.98, 0.99, 0.73, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.97, 0.99, 0.97, 0.25, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.18, 0.51, 0.71, 0.99, 0.99, 0.81, 0.01, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15, 0.58, 0.89, 0.99, 0.99, 0.99,\n",
       "         0.98, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.09, 0.45,\n",
       "         0.86, 0.99, 0.99, 0.99, 0.99, 0.79, 0.30, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.09, 0.26, 0.83, 0.99, 0.99, 0.99, 0.99, 0.77, 0.32, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.67, 0.86, 0.99, 0.99, 0.99, 0.99, 0.76, 0.31, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.21, 0.67, 0.88, 0.99, 0.99, 0.99, 0.99, 0.95, 0.52, 0.04, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.53, 0.99, 0.99, 0.99, 0.83, 0.53,\n",
       "         0.52, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00,\n",
       "         0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, shuffle=False): self.n, self.shuffle = len(ds), shuffle\n",
    "    def __iter__(self):\n",
    "        res = list(range(self.n))\n",
    "        if(self.shuffle): random.shuffle(res)\n",
    "        return iter(res)\n",
    "\n",
    "class BatchSampler():\n",
    "    def __init__(self, sampler, bs, drop_last=False): fc.store_attr()\n",
    "    def __iter__(self): yield from fc.chunked(iter(self.sampler), self.bs, drop_last=self.drop_last)\n",
    "\n",
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, batches, collate_fn=collate):fc.store_attr()\n",
    "    def __iter__(self): yield from (self.collate_fn(self.ds[i] for i in b) for b in self.batches)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badCollate(b): return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(Sampler(train_ds, shuffle=True), bs)\n",
    "valid_samp = BatchSampler(Sampler(valid_ds, shuffle=False), bs)\n",
    "train_dl = DataLoader(train_ds, train_samp)\n",
    "valid_dl = DataLoader(valid_ds, valid_samp)\n",
    "bad_train_dl = DataLoader(train_ds, train_samp, collate_fn=badCollate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "zip(*iterables, strict=False) --> Yield tuples until an input is exhausted.\n",
      "\n",
      "   >>> list(zip('abcdefg', range(3), range(4)))\n",
      "   [('a', 0, 0), ('b', 1, 1), ('c', 2, 2)]\n",
      "\n",
      "The zip object yields n-length tuples, where n is the number of iterables\n",
      "passed as positional arguments to zip().  The i-th element in every tuple\n",
      "comes from the i-th iterable argument to zip().  This continues until the\n",
      "shortest argument is exhausted.\n",
      "\n",
      "If strict is true and one of the arguments is exhausted before the others,\n",
      "raise a ValueError.\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "? zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "1 2 3\n",
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "print(a)\n",
    "print(*a)\n",
    "print(a[0], a[1], a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 8, 7, 2, 1, 3, 8, 6, 7, 8, 4, 1, 6, 1, 9, 4, 0, 9, 6, 2, 9, 3, 5, 9, 3, 0, 6, 1, 7, 5, 1, 8, 9, 1, 1, 5, 7, 4, 6, 4, 4, 0, 0, 4,\n",
       "        2, 3, 7, 0, 8, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = zip(*next(iter(bad_train_dl)))\n",
    "torch.stack(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([7, 1, 5, 3, 2, 6, 7, 2, 6, 2, 7, 3, 7, 0, 6, 3, 2, 5, 1, 8, 9, 2, 7, 7, 2, 2, 0, 4, 9, 6, 1, 1, 2, 7, 5, 4, 4, 4, 1, 9, 0, 1, 4, 8,\n",
       "         8, 9, 7, 2, 5, 0]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[next(iter(train_samp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([8, 9, 8, 4, 6, 4, 6, 0, 7, 8, 1, 7, 8, 9, 7, 5, 6, 4, 4, 9, 4, 7, 5, 9, 7, 6, 0, 1, 4, 4, 0, 7, 8, 6, 1, 6, 0, 4, 5, 8, 9, 5, 1, 2,\n",
       "         7, 6, 8, 4, 7, 3]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate(train_ds[i] for i in next(iter(train_samp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        report(loss, preds, yb)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc, count = 0., 0., 0\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred,yb).item()*n\n",
    "                tot_acc  += accuracy (pred,yb).item()*n\n",
    "            print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = BatchSampler(RandomSampler(train_ds), bs, drop_last=False)\n",
    "valid_samp = BatchSampler(SequentialSampler(valid_ds), bs, drop_last=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds)\n",
    "valid_dl = DataLoader(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, shuffle=False, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14, 0.96\n",
      "0 0.19284493958577514 0.946200003027916\n",
      "0.31, 0.92\n",
      "1 0.15838701024651528 0.9539000064134597\n",
      "0.15, 0.96\n",
      "2 0.12284666760824621 0.9626000046730041\n",
      "0.08, 0.94\n",
      "3 0.10450778256170451 0.968800008893013\n",
      "0.15, 0.94\n",
      "4 0.10737767970189452 0.9667000055313111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10737767970189452, 0.9667000055313111)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
